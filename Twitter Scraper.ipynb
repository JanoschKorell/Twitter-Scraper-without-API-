{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68701dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from selenium.common import exceptions\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from scipy import nan\n",
    "import pickle\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b276b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "############################################Functions###############################################\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0fcfe-28d1-485b-aaea-0bf00717f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903998d-ca77-44b6-adc9-f2a0ab869f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driverFirefox():\n",
    "    geckodriver = r'/Users/janoschkorell/Desktop/Wissenschaft/Statistik/Python/Python Test Projekte/Webscrape Social/geckodriver'\n",
    "    firefox_options = Options()\n",
    "    firefox_options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(executable_path = geckodriver, options = firefox_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8392ea-2646-40de-a5ce-dd7cb9751c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6a9a6-a40a-4cd2-b0e0-75c47fdc050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cookies(driver):\n",
    "\n",
    "    \"\"\"Function that clicks on each cookie popup\"\"\"\n",
    "    \n",
    "    try:\n",
    "\n",
    "        cookie = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Allen zustimmen')]\")\n",
    "        for x in cookie:\n",
    "            x.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # Neue Seite auslesen\n",
    "        url = driver.current_url\n",
    "        #print('Cookie Version 1')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        cookie = driver.find_elements(By.XPATH, \"//*[contains(@id, 'cookie')]\")\n",
    "        for x in cookie:\n",
    "            x.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # Neue Seite auslesen\n",
    "        url = driver.current_url\n",
    "       # print('Cookie Version 2')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        cookie = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Erforderliche und optionale Cookies erlauben')]\")\n",
    "        for x in cookie:\n",
    "            x.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # Neue Seite auslesen\n",
    "        url = driver.current_url\n",
    "       # print('Cookie Version 2')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        cookie = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Alle akzeptieren')]\")\n",
    "        for x in cookie:\n",
    "            x.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # Neue Seite auslesen\n",
    "        url = driver.current_url\n",
    "       # print('Cookie Version 3')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        cookie = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Alles akzeptieren')]\")\n",
    "        for x in cookie:\n",
    "            x.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # Neue Seite auslesen\n",
    "        url = driver.current_url\n",
    "        #print('Cookie Version 4')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        cookie = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Akzeptieren')]\")\n",
    "        for x in cookie:\n",
    "            x.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # Neue Seite auslesen\n",
    "        url = driver.current_url\n",
    "       # print('Cookie Version 5')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        cookie = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Alle Cookies akzeptieren')]\")\n",
    "        for x in cookie:\n",
    "            x.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # Neue Seite auslesen\n",
    "        url = driver.current_url\n",
    "       # print('Cookie Version 5')\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8537b-0de7-4227-a12d-c0447d7389d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice_pop_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a1549-df33-4cb7-9bb0-66e4f3fa0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notice_pop_up(driver):\n",
    "    \n",
    "    \"\"\"Function that clicks on the message pop up\"\"\"\n",
    "\n",
    "    try:\n",
    "        mitteilung = driver.find_element(By.XPATH, \"//div//div[@class='css-1dbjc4n r-13qz1uu']//div[2]\")\n",
    "        mitteilung.click()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        url = driver.current_url\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa02028-8f71-43e0-8887-6c12b2aaae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5a253-fc27-4a43-a37c-2375d4d69666",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "############################################Main Function###############################################\n",
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59744c1-0f36-4fac-bd30-afdd6818ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(Year_from, Month_from, Day_from, Year_until, Month_until, Day_until, Person, load_page, thoroughness):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Create a daily interval. Extract all tweets within the interval. Detect tweets, retweets and quotes and their dates. \n",
    "    Extract text and details of likes, retweets and replies. Splitting the quote into tweet and quote. \n",
    "    Recognizing who was quoted by.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #Initializes driver\n",
    "    driver = driverFirefox()\n",
    "    driver.get(f'https://twitter.com')\n",
    "    sleep(3)\n",
    "    cookies(driver)\n",
    "\n",
    "    \n",
    "    if continue_ == False:\n",
    "\n",
    "        y_tweet = 0\n",
    "\n",
    "        d = y_tweet\n",
    "        afile = open(r'y_tweet', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "        Tweets_Df_L = []\n",
    "\n",
    "        d = Tweets_Df_L\n",
    "        afile = open(r'Tweets_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        Links_Df_L = []\n",
    "\n",
    "        d = Links_Df_L\n",
    "        afile = open(r'Links_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        retweetet_Df_L = []\n",
    "\n",
    "        d = retweetet_Df_L\n",
    "        afile = open(r'retweetet_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "        retweets_citation_User_Df_L = []\n",
    "\n",
    "        d = retweets_citation_User_Df_L\n",
    "        afile = open(r'retweets_citation_User_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "        Tweet_Df_L = []\n",
    "\n",
    "        d = Tweet_Df_L\n",
    "        afile = open(r'Tweet_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "        citation_Re_Df_L = []\n",
    "\n",
    "        d = citation_Re_Df_L\n",
    "        afile = open(r'citation_Re_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        date_tweet_Df_L = []\n",
    "\n",
    "        d = date_tweet_Df_L\n",
    "        afile = open(r'date_tweet_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        date_citation_Df_L = []\n",
    "\n",
    "        d = date_citation_Df_L\n",
    "        afile = open(r'date_citation_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        date_retweet_Df_L = []\n",
    "\n",
    "        d = date_retweet_Df_L\n",
    "        afile = open(r'date_retweet_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        datetime_bis_Df_L = []\n",
    "\n",
    "        d = datetime_bis_Df_L\n",
    "        afile = open(r'datetime_bis_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        datetime_from_Df_L = []\n",
    "\n",
    "        d = datetime_from_Df_L\n",
    "        afile = open(r'datetime_from_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        replies_quantity_Df_L = []\n",
    "\n",
    "        d = replies_quantity_Df_L\n",
    "        afile = open(r'replies_quantity_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "        retweets_quantity_Df_L = []\n",
    "\n",
    "        d = retweets_quantity_Df_L\n",
    "        afile = open(r'retweets_quantity_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "\n",
    "        likes_quantity_Df_L = []\n",
    "\n",
    "\n",
    "        d = likes_quantity_Df_L\n",
    "        afile = open(r'likes_quantity_Df_L', 'wb')\n",
    "        pickle.dump(d, afile)\n",
    "        afile.close()\n",
    "\n",
    "\n",
    "    if continue_ == True:\n",
    "        \n",
    "        \n",
    "        del Year_from, Month_from, Day_from\n",
    "        \n",
    "        \n",
    "        #continue with the existing results \n",
    "        \n",
    "        file2 = open(r'retweets_citation_User_Df_L', 'rb')\n",
    "        retweets_citation_User_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'retweetet_Df_L', 'rb')\n",
    "        retweetet_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'y_tweet', 'rb')\n",
    "        y_tweet = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'Tweets_Df_L', 'rb')\n",
    "        Tweets_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'Links_Df_L', 'rb')\n",
    "        Links_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'Tweet_Df_L', 'rb')\n",
    "        Tweet_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'citation_Re_Df_L', 'rb')\n",
    "        citation_Re_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'date_tweet_Df_L', 'rb')\n",
    "        date_tweet_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'date_citation_Df_L', 'rb')\n",
    "        date_citation_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'date_retweet_Df_L', 'rb')\n",
    "        date_retweet_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'datetime_bis_Df_L', 'rb')\n",
    "        datetime_bis_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "        file2 = open(r'datetime_from_Df_L', 'rb')\n",
    "        datetime_from_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'replies_quantity_Df_L', 'rb')\n",
    "        replies_quantity_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "\n",
    "        file2 = open(r'retweets_quantity_Df_L', 'rb')\n",
    "        retweets_quantity_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "        file2 = open(r'likes_quantity_Df_L', 'rb')\n",
    "        likes_quantity_Df_L = pickle.load(file2)\n",
    "        file2.close()\n",
    "        \n",
    "        file2 = open(r'current_datetime_from', 'rb')\n",
    "        datetime_from = pickle.load(file2)\n",
    "        file2.close()\n",
    "\n",
    "        \n",
    "        try:\n",
    "            reg2 = r\"(\\d+)-(\\d\\d)-(\\d\\d)\"\n",
    "            RegTextdate = re.search(reg2, datetime_from, re.DOTALL)\n",
    "            Year_from = int(RegTextdate.group(1))\n",
    "            Month_from = int(RegTextdate.group(2))\n",
    "            Day_from = int(RegTextdate.group(3))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        print('Continue: ' + datetime_from)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Loops until date_from == date_until. \n",
    "    #You have to think of the period from the end: the direction is always from a point in time in the past\n",
    "    #It is only possible to specify time periods. One includes the search day with an interval.\n",
    "    while True:\n",
    "\n",
    "        #Transforms the input into the correct form\n",
    "        #Part 1 of the interval to string\n",
    "        if Month_from < 10:\n",
    "            Month_from_str = '0' + str(Month_from)\n",
    "        else:\n",
    "            Month_from_str = str(Month_from)\n",
    "\n",
    "        if Day_from < 10:\n",
    "            Day_from_str = '0' + str(Day_from)\n",
    "        else:\n",
    "            Day_from_str = str(Day_from)\n",
    "\n",
    "        Year_from_str = str(Year_from)\n",
    "\n",
    "\n",
    "        #Part 2 of the interval\n",
    "        Day_from_2 = Day_from - 1\n",
    "        Month_from_2 = Month_from\n",
    "        Year_from_2 = Year_from\n",
    "\n",
    "        #Each month is considered to have 31 days. If the month is at 0, it is set back to 31.\n",
    "        if Day_from_2 == 0:\n",
    "            Day_from_2 = 31\n",
    "            Month_from_2 = Month_from_2 -1\n",
    "            if Month_from_2 == 0:\n",
    "                Month_from_2 = 12\n",
    "                Year_from_2 = Year_from_2 -1\n",
    "\n",
    "                \n",
    "        #Transforms the part 2 of the interval into the correct form and to string\n",
    "        if Month_from_2 < 10:\n",
    "            Month_from_str_2 = '0' + str(Month_from_2)\n",
    "        else:\n",
    "            Month_from_str_2 = str(Month_from_2)\n",
    "\n",
    "        if Day_from_2 < 10:\n",
    "            Day_from_str_2 = '0' + str(Day_from_2)\n",
    "        else:\n",
    "            Day_from_str_2 = str(Day_from_2)\n",
    "\n",
    "        Year_from_str_2 = str(Year_from_2)\n",
    "\n",
    "        #Final form\n",
    "        datetime_from =  Year_from_str + '-' + Month_from_str + '-' + Day_from_str\n",
    "        print('Currently scraping: ', datetime_from)\n",
    "        datetime_from_2 =  Year_from_str_2  + '-' + Month_from_str_2 + '-' + Day_from_str_2\n",
    "\n",
    "        notice_pop_up(driver)\n",
    "        Tweets_temp_L = []\n",
    "        Links_temp_L = []\n",
    "        Ant_Likes_Ret_temp_L = []\n",
    "\n",
    "        #Loop for the current day\n",
    "        \n",
    "        #for loop\n",
    "        #A big problem with Twitter, is that it can happen that the page does not load properly and \n",
    "        #tweets cannot be captured. Therefore, the page is called again and again until it is loaded. \n",
    "        #For this reason, there is this \"for loop\". \n",
    "        \n",
    "        for i in range(0,thoroughness):\n",
    "            driver.get(f'https://twitter.com/search?q=(from%3A%40{Person})%20until%3A{datetime_from}%20since%3A{datetime_from_2}%20%20include%3Anativeretweets%20exclude%3Areplies')\n",
    "            sleep(load_page)\n",
    "            cookies(driver)\n",
    "            notice_pop_up(driver)\n",
    "            \n",
    "\n",
    "\n",
    "            #while loop in for loop\n",
    "            #Twitter loads the tweets only gradually. \n",
    "            #Thus, the tweets have to be extracted one after the other by scrolling the screen down step by step.\n",
    "            #The following while loop accomplishes the step-by-step extraction.\n",
    "            \n",
    "            n = 1\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                \n",
    "                #Extrahiert tweet text, tweet link und weitere Angaben\n",
    "                Tweets_ = driver.find_elements(By.XPATH,\"//div[@data-testid='cellInnerDiv']//article[@data-testid='tweet']\")\n",
    "                Links_ = driver.find_elements(By.XPATH,\"//div[@data-testid='cellInnerDiv']//article//a[@class='css-4rbku5 css-18t94o4 css-901oao r-1bwzh9t r-1loqt21 r-xoduu5 r-1q142lx r-1w6e6rj r-1qd0xha r-a023e6 r-16dba41 r-9aw3ui r-rjixqe r-bcqeeo r-3s2u2q r-qvutc0']\")\n",
    "                Ant_Likes_Ret_ = driver.find_elements(By.XPATH,\"//div[@class='css-1dbjc4n r-1ta3fxp r-18u37iz r-1wtj0ep r-1s2bzr4 r-1mdbhws']\")\n",
    "\n",
    "\n",
    "                #Extracts selenium web element into text and link\n",
    "                for x in Links_:\n",
    "                    try:\n",
    "                        link =  x.get_attribute('href')\n",
    "                        Links_temp_L.append(link)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                for x in Tweets_:\n",
    "                    try:\n",
    "                        tweet =  x.text\n",
    "                        Tweets_temp_L.append(tweet)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "\n",
    "                for x in Ant_Likes_Ret_:\n",
    "                    try:\n",
    "                        k_ant_like =  x.get_attribute('aria-label')\n",
    "                    except:\n",
    "                        k_ant_like = []\n",
    "                    Ant_Likes_Ret_temp_L.append(k_ant_like)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                sleep(load_page)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                notice_pop_up(driver)\n",
    "\n",
    "                # Calculate new scroll height and compare with last scroll height\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                # break condition\n",
    "                if new_height == last_height:\n",
    "                    n = n+1\n",
    "                last_height = new_height\n",
    "\n",
    "                notice_pop_up(driver)\n",
    "                if n == 3:\n",
    "                    break\n",
    "\n",
    "            notice_pop_up(driver)\n",
    "            \n",
    "            #The for loop ends when selenium has found something.\n",
    "            if Links_temp_L != []:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Delete duplicates\n",
    "        Links_ = []\n",
    "        for x in Links_temp_L:\n",
    "                if x not in Links_:\n",
    "                    Links_.append(x)\n",
    "\n",
    "\n",
    "\n",
    "        Tweets_ = []\n",
    "        for x in Tweets_temp_L:\n",
    "                if x not in Tweets_:\n",
    "                    Tweets_.append(x)\n",
    "\n",
    "\n",
    "\n",
    "        Ant_Likes_Ret_ = []\n",
    "        for x in Ant_Likes_Ret_temp_L:\n",
    "                if x not in Ant_Likes_Ret_:\n",
    "                    Ant_Likes_Ret_.append(x)\n",
    "\n",
    "\n",
    "        #If there are problems with the extraction, the complete loop ends. \n",
    "        if len(Tweets_) != len(Links_) != len(Ant_Likes_Ret_):\n",
    "            print('Nicht alles erfasst')\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        #Further processing     \n",
    "        for tweet,link, Ant_Likes_Ret in zip(Tweets_, Links_,Ant_Likes_Ret_):\n",
    "\n",
    "            #Number of tweets\n",
    "            y_tweet = y_tweet + 1\n",
    "\n",
    "            #Lists for df\n",
    "            Zitat_Re_Text = []\n",
    "            Tweet_text = []\n",
    "            datum_retweet = []\n",
    "            datum_tweet = []\n",
    "            datum_zitat = []\n",
    "\n",
    "            tweetT = tweet\n",
    "            linkT = link\n",
    "\n",
    "            if bool(linkT in Links_Df_L) == False:\n",
    "\n",
    "                if bool(tweetT.startswith('Dieser Tweet ist nicht verfügbar')) == False:\n",
    "\n",
    "            \n",
    "                    Tweets_Df_L.append(tweetT)\n",
    "                    Links_Df_L.append(linkT)\n",
    "\n",
    "                    \n",
    "                    #The decision of whether it is your own tweet, a retweet or a quote is made in the following way:\n",
    "\n",
    "                    #1. starting point is \"no\", i.e. own tweet\n",
    "                    #2. if the word \"retweeted\" appears in the tweet, then it is a retweet.\n",
    "                    #3. if the words \"quote tweet\" appear in a tweet, then it is a quote. \n",
    "\n",
    "            \n",
    "                    retweetet = 'no'\n",
    "                    if bool('retweetet' in tweetT) == True:\n",
    "                        retweetet = 'retweet'\n",
    "\n",
    "                    #Date of tweets, retweets and citations\n",
    "                    reg3 = r\"\\d+\\.\\s\\w+\\.?\\s?\\s?\\d?\\d?\\d?\\d?\\n\"\n",
    "                    ReCheckText = re.findall(reg3, tweetT, re.MULTILINE)\n",
    "                    if len(ReCheckText) == 1 and retweetet == 'retweet':\n",
    "                        datum_retweet = ReCheckText[0]\n",
    "                    if len(ReCheckText) == 1 and retweetet == 'no':\n",
    "                        datum_tweet = ReCheckText[0]\n",
    "                    if len(ReCheckText) == 2 :\n",
    "                        datum_tweet = ReCheckText[0]\n",
    "                        datum_zitat = ReCheckText[1]\n",
    "\n",
    "                    date_retweet_Df_L.append(datum_retweet)\n",
    "                    date_tweet_Df_L.append(datum_tweet)\n",
    "                    date_citation_Df_L.append(datum_zitat)\n",
    "\n",
    "                    #Find all other profiles mentioned\n",
    "                    reg = r\"\\@.*?(?=[\\s\\n\\.]+)\"   #r\"\\@.*?(?=[\\s\\.\\n]+\\d+\\.\\s\\w+)\"\n",
    "                    ReZit = re.findall(reg,tweetT, re.DOTALL | re.MULTILINE)\n",
    "                    zitate_reTweets_Df_L = []\n",
    "                    for x in ReZit:\n",
    "                        x = re.sub('\\.', '', x).strip()\n",
    "                        x = re.sub('\\n', '', x).strip()\n",
    "                        x = re.sub('\\·', '', x).strip()\n",
    "                        x = re.sub(' ', '', x).strip()\n",
    "                        if x == '@'+Person:\n",
    "                            del x\n",
    "                    try:\n",
    "                        zitate_reTweets_Df_L.append(x)\n",
    "                        x = x\n",
    "                    except:\n",
    "                        x = nan\n",
    "\n",
    "                    retweets_citation_User_Df_L.append(x)\n",
    "\n",
    "                    #Texte Zitate Retweets und Tweets\n",
    "                    #if str(x) != str(nan):\n",
    "\n",
    "                    \n",
    "                   #Extracting the texts. Quotes are split into tweet and citation.\n",
    "                    \n",
    "                    \n",
    "                        #Retweets Text\n",
    "                       # if len(ReCheckText) == 1:\n",
    "                    if 'retweetet' in tweetT:\n",
    "                        try:\n",
    "                            reg2 = r\"\\@*[A-Za-zöäü\\_0-9\\n\\·]+\\d+\\.\\s\\w+\\.?\\s?\\d?\\d?\\d?\\d?\\n(.*?)(?=\\d+\\.\\d+|[0-9]+\\n\\d|Diesen Thread anzeigen)\"\n",
    "                            RegTextRetweet = re.search(reg2, tweetT, re.DOTALL)\n",
    "                            Zitat_Re_Text = RegTextRetweet.group(1)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "\n",
    "               # if len(ReCheckText) == 2:\n",
    "                    if \"Tweet zitieren\" in tweetT:\n",
    "                        retweetet = 'citation'\n",
    "                        try:\n",
    "                            reg5 = r\"\\@*[A-Za-zöäü\\_0-9\\n\\·\\s\\*]+\\d+\\.\\s\\w+\\.?\\s?\\d?\\d?\\d?\\d?\\n(.*?)\\n(?=[A-Za-zöäü\\-\\_0-9\\s\\*\\.]+\\n\\@[A-Za-zöäü\\_0-9\\n\\·\\.\\s\\-\\*]+|Diesen Thread anzeigen|\\n?\\d+[0-9\\.]+\\n|\\d+\\:\\d+|Tweet zitieren)\"\n",
    "                            RegTextRetweet_Zitate = re.findall(reg5, tweetT, re.DOTALL | re.MULTILINE)\n",
    "                            Tweet_text = RegTextRetweet_Zitate[0]\n",
    "                            Zitat_Re_Text = RegTextRetweet_Zitate[1]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "\n",
    "                    if len(ReCheckText) != 2:\n",
    "                        if \"Tweet zitieren\" not in tweetT:\n",
    "                            if 'retweetet' not in tweetT:\n",
    "                                reg6 = r\"\\@*[A-Za-zöäü\\_0-9\\n\\·\\s]+\\d+\\.\\s\\w+\\.?\\s?\\d?\\d?\\d?\\d?\\n(.*?)(?=Diesen Thread anzeigen|\\n[0-9\\.]+\\n)\"\n",
    "                                Tweet_reg = re.search(reg6, tweetT, re.DOTALL | re.MULTILINE)\n",
    "                                Tweet_text = Tweet_reg.group(1)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    #Number of likes,retweets,quotes\n",
    "                    Ant_Likes_RetT = Ant_Likes_Ret\n",
    "\n",
    "                    try:\n",
    "                        regAnt = r\"([0-9\\.]+)\\sAntworten\"\n",
    "                        Ant = re.search(regAnt, Ant_Likes_RetT, re.DOTALL)\n",
    "                        antworten_anzahl = int(Ant.group(1))\n",
    "                    except:\n",
    "                        antworten_anzahl = 0\n",
    "\n",
    "                    try:\n",
    "                        regRet = r\"([0-9\\.]+)\\sRetweets\"\n",
    "                        Ret = re.search(regRet, Ant_Likes_RetT, re.DOTALL)\n",
    "                        retweets_anzahl = int(Ret.group(1))\n",
    "                    except:\n",
    "                        retweets_anzahl = 0\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        regLikes = r\"([0-9\\.]+)\\s„Gefällt mir“\"\n",
    "                        Likes = re.search(regLikes, Ant_Likes_RetT, re.DOTALL)\n",
    "                        likes_anzahl = int(Likes.group(1))\n",
    "                    except:\n",
    "                        likes_anzahl = 0\n",
    "\n",
    "\n",
    "                    #Collect all extractions in lists\n",
    "                    Tweet_Df_L.append(Tweet_text)\n",
    "                    citation_Re_Df_L.append(Zitat_Re_Text)\n",
    "                    retweetet_Df_L.append(retweetet)\n",
    "                    datetime_from_Df_L.append(datetime_from)\n",
    "                    datetime_bis_Df_L.append(datetime_from_2)\n",
    "\n",
    "                    replies_quantity_Df_L.append(antworten_anzahl)\n",
    "                    retweets_quantity_Df_L.append(retweets_anzahl)\n",
    "                    likes_quantity_Df_L.append(likes_anzahl)\n",
    "                    \n",
    "                    \n",
    "                    #Save current results \n",
    "                    d = y_tweet\n",
    "                    afile = open(r'y_tweet', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = Tweets_Df_L\n",
    "                    afile = open(r'Tweets_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = Links_Df_L\n",
    "                    afile = open(r'Links_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = retweetet_Df_L\n",
    "                    afile = open(r'retweetet_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = retweets_citation_User_Df_L\n",
    "                    afile = open(r'retweets_citation_User_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = Tweet_Df_L\n",
    "                    afile = open(r'Tweet_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = citation_Re_Df_L\n",
    "                    afile = open(r'citation_Re_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = date_tweet_Df_L\n",
    "                    afile = open(r'date_tweet_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = date_citation_Df_L\n",
    "                    afile = open(r'date_citation_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = date_retweet_Df_L\n",
    "                    afile = open(r'date_retweet_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = datetime_bis_Df_L\n",
    "                    afile = open(r'datetime_bis_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = datetime_from_Df_L\n",
    "                    afile = open(r'datetime_from_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = replies_quantity_Df_L\n",
    "                    afile = open(r'replies_quantity_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = retweets_quantity_Df_L\n",
    "                    afile = open(r'retweets_quantity_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "                    d = likes_quantity_Df_L\n",
    "                    afile = open(r'likes_quantity_Df_L', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "                    \n",
    "                    d = datetime_from\n",
    "                    afile = open(r'current_datetime_from', 'wb')\n",
    "                    pickle.dump(d, afile)\n",
    "                    afile.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "        #The first \"while loop\" ends when the \"date from\" and the \"date until\" are equal. \n",
    "        if bool(int(Day_from_str) == Day_until and int(Month_from_str) == Month_until and int(Year_from_str) == Year_until) == True:\n",
    "            break\n",
    "\n",
    "\n",
    "        #The int form of the date. One day is subtracted per pass.\n",
    "        Day_from = Day_from - 1\n",
    "\n",
    "        if Day_from == 0:\n",
    "            Day_from = 31\n",
    "            Month_from = Month_from -1\n",
    "            if Month_from == 0:\n",
    "                Month_from = 12\n",
    "                Year_from = Year_from -1\n",
    "\n",
    "\n",
    "\n",
    "    #All Lists to df\n",
    "    Twitter_Df = pd.DataFrame()\n",
    "\n",
    "    Twitter_Df['Retweet/Citation'] = retweetet_Df_L\n",
    "    Twitter_Df['Tweet'] = Tweet_Df_L\n",
    "    Twitter_Df['Retweet/Citation_Text'] = citation_Re_Df_L\n",
    "    Twitter_Df['Retweet/Citation_User'] = retweets_citation_User_Df_L\n",
    "\n",
    "    Twitter_Df['Date_Tweet'] = date_tweet_Df_L\n",
    "    Twitter_Df['Date_Retweet'] = date_retweet_Df_L\n",
    "    Twitter_Df['Date_Citation'] = date_citation_Df_L\n",
    "\n",
    "\n",
    "    Twitter_Df['Answers_Quantity'] = replies_quantity_Df_L\n",
    "    Twitter_Df['Retweets_Quantity'] = retweets_quantity_Df_L\n",
    "    Twitter_Df['Likes_Quantity'] = likes_quantity_Df_L\n",
    "\n",
    "    Twitter_Df['Datetime_from'] = datetime_bis_Df_L\n",
    "    Twitter_Df['Datetime_until'] = datetime_from_Df_L\n",
    "\n",
    "    Twitter_Df['Links'] = Links_Df_L\n",
    "    Twitter_Df['Tweets_complete'] = Tweets_Df_L\n",
    "    \n",
    "    Twitter_Df['Tweets_complete'] = Twitter_Df['Tweets_complete'].drop_duplicates()\n",
    "    \n",
    "    #Export of df as csv\n",
    "    Twitter_Df.to_csv('Scraped_Tweets.csv')\n",
    "    \n",
    "    \n",
    "    return Twitter_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032405e3-1854-4368-80fd-b869036940f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "############################################Specifications###############################################\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cd5d4-f696-4a3f-ab48-0ed1654c733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here you can enter your data:\n",
    "\n",
    "\n",
    "#from\n",
    "\n",
    "#Type in year\n",
    "Year_from =      #<---- int \n",
    "#Type in month\n",
    "Month_from =     #<---- int \n",
    "#Type in day\n",
    "Day_from =       #<---- int \n",
    "\n",
    "\n",
    "\n",
    "#until\n",
    "\n",
    "#Type in year\n",
    "Year_until =     #<---- int\n",
    "#Type in month\n",
    "Month_until =    #<---- int\n",
    "#Type in day\n",
    "Day_until =      #<---- int\n",
    "\n",
    "#Continue Extraktion (True / False)\n",
    "continue_ = False\n",
    "\n",
    "#The higher the values, the more thoroughly the extraction runs, but the slower it is.\n",
    "#seconds\n",
    "load_page = 2  \n",
    "thoroughness = 14\n",
    "\n",
    "\n",
    "#Person (must be the name as it appears in Twitter)\n",
    "Person = ' '      #<----- string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3b61e-83e8-4080-9075-9a229d076af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tweets(Year_from, Month_from, Day_from, Year_until, Month_until, Day_until, Person, load_page, thoroughness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
